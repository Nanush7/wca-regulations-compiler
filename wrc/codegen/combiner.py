import re
import json
from bs4 import BeautifulSoup
from exceptions import CombinerBadFormat, CombinerNotFound

FIRST_LINE_FORMAT = '<% provide(:title, \'{0}\') %>'
TITLE_ENGLISH = 'WCA Regulations and Guidelines'
TITLE_TRANSLATION = '{0} {1} {2} WCA ({3})'
NATIVE = 'native'
NATIVE_REGULATIONS = 'regulations'
NATIVE_GUIDELINES = 'guidelines'
NATIVE_AND = 'and'


class Combiner:
    """
    Class to combine the WCA Regulations and Guidelines into a single HTML page.
    The HTML page is generated by dynamically appending the lines of each document to `self.combined` as a string.
    """
    def __init__(self, regs_path, guides_path, lang='english'):
        self.regs_soup = None
        self.guides_soup = None
        self.combined = ''

        self._get_soup(regs_path, guides_path)

        # We ignored the first line in _get_soup, now we manually add it.
        if lang == 'english':
            title = TITLE_ENGLISH
        else:
            # First we load the native words from the language.
            with open('data/languages.json', 'r') as file:
                try:
                    language = json.loads(file.read())[lang]
                except KeyError as e:
                    print('Combiner Error >> Invalid language or internal key error.')
                    raise e

            # Now we make the title line based on the language.
            title = TITLE_TRANSLATION.format(language[NATIVE_REGULATIONS],
                                             language[NATIVE_AND],
                                             language[NATIVE_REGULATIONS],
                                             language[NATIVE])

        self.combined = FIRST_LINE_FORMAT.format(title)
        self.information = self.get_information(lang)

    def _get_soup(self, regs_path: str, guides_path: str) -> None:
        """
        Get BS4 objects.
        """
        # Read files and get soup.
        with open(regs_path, 'r') as file:
            # We don't save the first line, to avoid problems with BS4.
            file.readline()
            regs_content = file.read()
        with open(guides_path, 'r') as file:
            file.readline()
            guides_content = file.read()
        self.regs_soup = BeautifulSoup(markup=regs_content, features='html.parser')
        self.guides_soup = BeautifulSoup(markup=guides_content, features='html.parser')

    def get_information(self, lang: str) -> str:
        """
        * Extract the beginning of the regulations file (up to the table of contents).
        * Combine the tables of contents in case either of the documents has more articles.
        * Handle special case for translations.
        """
        information = ''

        # Generate the new table of contents.
        toc_regs = self.regs_soup.find('ul', attrs={'id': 'table_of_contents'})
        toc_guides = self.guides_soup.find('ul', attrs={'id': 'table_of_contents'})
        # Check if it found the elements.
        if toc_regs is None or toc_guides is None:
            print('Combiner >> table_of_contents not found.')
            raise CombinerNotFound('table_of_contents')

        # Utilizar conjuntos para realizar un seguimiento de los enlaces Ãºnicos
        enlaces_set = {a['href'] for a in toc_regs.find_all('a')}
        enlaces_set.update({a['href'] for a in toc_guides.find_all('a')})

        # Ordenar los enlaces y luego imprimirlos
        enlaces_ordenados = sorted(enlaces_set)

        for enlace in enlaces_ordenados:
            print(enlace)

        return information


if __name__ == '__main__':
    combiner = Combiner('../../build/index.html.erb', '../../build/guidelines.html.erb')
